Sachin Natesh - Homework 4 Responses

------------------------------------#### NOTE ####------------------------------------------------

- For comparing between graphics cards, I use my computer with an NVIDIA GeForce 1060 
  graphics card with 1280 CUDA cores, and the cuda1/cuda2 cims GPU nodes. 
- Timings for 2D Jacobi are conducted on my machine with 1000 iterations. 
- In all cases, the CPU time uses an OpenMP implementation with the thread count given by OMP_NUM_THREADS.
- I'm a CUDA noob and couldn't generalize parallel reduction with dot product to mat-vec without using
  dynamic parallelism features. The performance gains are only seen for large N, and are marginal.
  If N = NUM_BLOCKS = BLOCK_SIZE, there is a simple kernel that gives the same performance as dot product,
  but I have not included it in the code.
- Because of the last point, the Makefile must use -sm_35 as a flag for nvcc on the cims machines

-------------------------#### Dot Product by parallel reduction ####------------------------------
My Machine:


cuda1:



cuda2:


--------------------#### Matrix Vector Product by *attempted* parallel reduction ####------------- 
My Machine:


cuda1:



cuda2:



-------------------------------------#### 2D Jacobi ####------------------------------------------

  N         CPU (s)         GPU (s)         Speedup         Res CPU         Res GPU          Error
 30        0.491777        0.290348         1.69375     0.000152309     0.000152309      1.0328e-09
 40         2.36218        0.475859         4.96403      0.00104569      0.00104568      3.6171e-05
 50         6.61751        0.806993         8.20022       0.0023793       0.0023793     2.59668e-06
 60         14.3218         1.14023         12.5605      0.00352413      0.00352407     0.000750887
 70          32.052         1.77498         18.0577      0.00428672      0.00428671      0.00021534
 80         128.505         2.52671         50.8585      0.00471607      0.00471601      0.00161782
 90         118.214         3.69206         32.0186      0.00491151      0.00491149     0.000819344
100         218.968         5.35449         40.8943      0.00495763      0.00495761      0.00111736



------------------------------------#### Final Project Update ####--------------------------------
I've made decent progress on my final project. As a reminder, I am implementing spreading and
interpolation routines to couple forces on lagrangian particles to a uniform Eulerian grid using
a compactly supported spreading kernel. I haven't run into any issues yet. So far, I've implemented: 
 
  1) An array-based linked-list data structure based on open addressing. This structure partitions
     particles randomly distributed throughout the Eulerian grid into columns (as viewed from the top).
     An array firstn(i,j) gives p_ind, the first particle in column(i,j) of the grid. Then, another
     array nextn(p_ind) gives the next particle in column(i,j).
  2) Gather and scatter operations so that I can exploit auto-vectorization during the spreading
     and interpolation loops over the columns in each group (all separated by kernel width * grid_spacing) 
